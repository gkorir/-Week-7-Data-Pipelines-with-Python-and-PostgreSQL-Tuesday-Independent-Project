{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Project Deliverable##\n",
        "\n",
        "A GitHub repository with a python file (.py) or notebook (.ipynb) with your solution."
      ],
      "metadata": {
        "id": "7HXkDuXR9Ite"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem statement##\n",
        "\n",
        "Telecom companies often have to extract billing data from multiple CSV files generated from\n",
        "various systems and transform it into a structured format for analysis and revenue reporting.\n",
        "\n",
        "Thus, there is a need for an automated data pipeline that\n",
        "can extract billing data from multiple sources and transform it into a structured format for\n",
        "efficient analysis and revenue reporting"
      ],
      "metadata": {
        "id": "VJQnNiIJ_V4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Guidelings to solving this problems##\n",
        "\n",
        "● Data Extraction: The data pipeline should be designed to collect data from various sources, including network sensors, equipment sensors, and maintenance records.\n",
        "\n",
        "● Data Transformation: The collected data must be cleaned and transformed to ensure consistency and quality. This will involve removing duplicates, fixing missing data, and normalizing the data for consistency.\n",
        "\n",
        "● Data Analysis: The cleaned data will be used to build machine learning models that can predict potential equipment failures and schedule maintenance proactively.\n",
        "\n",
        "\n",
        "● Data Loading: The resulting data will be stored in a PostgreSQL database.\n",
        "\n",
        "\n",
        "\n",
        "Data set provided : (https://bit.ly/3YNdO2Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The datasets will be in CSV format and will include the following fields:\n",
        "\n",
        "\n",
        " 1.Equipment sensor data: ID, date, time, sensor reading\n",
        "\n",
        " 2.Network sensor data: ID, date, time, sensor reading\n",
        "\n",
        " 3.Maintenance records: ID, date, time, equipment ID, maintenance type\n"
      ],
      "metadata": {
        "id": "LMwJ7RHQNtL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing required libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "LJbe_EDpQla0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to establish connection to the sql DB"
      ],
      "metadata": {
        "id": "xBE96Ob8RaAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "POSTGRES_ADDRESS = '22.237.226.11'\n",
        "POSTGRES_PORT = '5422'\n",
        "POSTGRES_USERNAME = 'postgresDB'\n",
        "POSTGRES_PASSWORD = 'password01'\n",
        "POSTGRES_DBNAME = 'telecommunications_data'"
      ],
      "metadata": {
        "id": "QjSJSgaRShX_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the database engine to be used for parsing "
      ],
      "metadata": {
        "id": "RMs6acZRVlqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the database engine instrumental for data parsing\n",
        "postgres_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'\n",
        "  .format(username=POSTGRES_USERNAME,\n",
        "     password=POSTGRES_PASSWORD,\n",
        "     ipaddress=POSTGRES_ADDRESS,\n",
        "     port=POSTGRES_PORT,\n",
        "     dbname=POSTGRES_DBNAME))\n",
        "engine = create_engine(postgres_str)"
      ],
      "metadata": {
        "id": "LudUC0o9VyIv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data extraction from data sources on various fields ie Equipment sensor data, Network sensor data, Maintenance records\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wYu-zOR3WoNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data():\n",
        "    # Load the equipment sensor data\n",
        "    equipment_df = pd.read_csv('equipment_sensor.csv')\n",
        "\n",
        "    # Loading the network sensor data\n",
        "    network_df = pd.read_csv('network_sensor.csv')\n",
        "\n",
        "    # Load the maintenance records data\n",
        "    maintenance_df = pd.read_csv('maintenance_records.csv')\n",
        "\n",
        "    return equipment_df, network_df, maintenance_df"
      ],
      "metadata": {
        "id": "mUJMYHVEeIO-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Transformation \n",
        "\n",
        "\n",
        "This involve removing duplicates, fixing missing data, normalizing the data for consistency, aggregation , joining and data enrichment."
      ],
      "metadata": {
        "id": "8vWaY5RLrqkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(equipment_df, network_df, maintenance_df):\n",
        "    # getting rid of the duplicates\n",
        "    equipment_df.drop_duplicates(inplace=True)\n",
        "    network_df.drop_duplicates(inplace=True)\n",
        "    maintenance_df.drop_duplicates(inplace=True)\n",
        "\n",
        "    # Dealing with missing data\n",
        "    equipment_df.fillna(method='ffill', inplace=True)\n",
        "    network_df.fillna(method='ffill', inplace=True)\n",
        "    maintenance_df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "    # Normalizing my data\n",
        "    equipment_df['date_time'] = pd.to_datetime(equipment_df['date'] + ' ' + equipment_df['time'])\n",
        "    equipment_df.drop(['date', 'time'], axis=1, inplace=True)\n",
        "\n",
        "    network_df['date_time'] = pd.to_datetime(network_df['date'] + ' ' + network_df['time'])\n",
        "    network_df.drop(['date', 'time'], axis=1, inplace=True)\n",
        "\n",
        "    maintenance_df['date_time'] = pd.to_datetime(maintenance_df['date'] + ' ' + maintenance_df['time'])\n",
        "    maintenance_df.drop(['date', 'time'], axis=1, inplace=True)\n",
        "\n",
        "    # Aggregating the data\n",
        "    equipment_summary = equipment_df.groupby('ID').agg({'date_time': ['min', 'max'], 'sensor_reading': ['mean', 'max']})\n",
        "    equipment_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']\n",
        "    network_summary = network_df.groupby('ID').agg({'date_time': ['min', 'max'], 'sensor_reading': ['mean', 'max']})\n",
        "    network_summary.columns = ['first_seen', 'last_seen', 'average_reading', 'max_reading']\n",
        "\n",
        "    # Joining the data from the equipment and network sensor data sets\n",
        "    sensor_summary = pd.merge(equipment_summary, network_summary, how='outer', left_index=True, right_index=True)\n",
        "    sensor_summary = sensor_summary.reset_index()\n",
        "    sensor_summary = sensor_summary.rename(columns={'ID': 'equipment_ID'})\n",
        "\n",
        "    maintenance_df = maintenance_df[['date_time', 'equipment_ID', 'maintenance_type']]\n",
        "\n",
        "    return sensor_summary, maintenance_df"
      ],
      "metadata": {
        "id": "HwydpvAssXKa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LqZnmUX45p1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(sensor_summary, maintenance_df):\n",
        "    # using to_sql function to Load the data to PostgreSQL\n",
        "    sensor_summary.to_sql('sensor_summary', engine, if_exists='replace')\n",
        "    maintenance_df.to_sql('maintenance_records', engine, if_exists='replace')"
      ],
      "metadata": {
        "id": "0FZb6-SZ6jg6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the main function\n",
        "\n",
        "We will call the main() from within another Python script or from the command line."
      ],
      "metadata": {
        "id": "rSD6yDvVCSVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    equipment_df, network_df, maintenance_df = extract_data()\n",
        "    sensor_summary, maintenance_df = transform_data(equipment_df, network_df, maintenance_df)\n",
        "    load_data(sensor_summary, maintenance_df)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "a9YHs7NqPcqq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}